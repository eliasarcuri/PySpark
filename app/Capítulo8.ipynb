{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea3bae-cc33-425b-8da2-4d98746158c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Caminho para os drivers JDBC dentro do contêiner Docker\n",
    "path_postgresql = \"/opt/spark/jars/postgresql-42.2.5.jar\"\n",
    "path_mysql = \"/opt/spark/jars/mysql-connector-java-8.0.29.jar\"\n",
    "path_sqlserver = \"/opt/spark/jars/sqljdbc42.jar\"\n",
    "\n",
    "# Configurando a SparkSession para incluir os drivers JDBC\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars\", f\"{path_postgresql},{path_mysql},{path_sqlserver}\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e97421-dd2f-44d8-9865-18a995816c1f",
   "metadata": {},
   "source": [
    "## Criar schema se precisar criar a tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a14d3f-fdd0-4c20-9230-fb28dfd07057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "| id|      nome|valor|\n",
      "+---+----------+-----+\n",
      "|  1|  Cliente1|100.0|\n",
      "|  1|  Cliente1|100.0|\n",
      "|  1|  Cliente1|100.0|\n",
      "|  1|  Cliente1|100.0|\n",
      "|  1|'Cliente1'|100.0|\n",
      "|  2|  Cliente2|100.0|\n",
      "+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conexão com o PostgreSQL em outro contêiner Docker\n",
    "postgres_url = \"jdbc:postgresql://postgres:5432/exemploDB\"\n",
    "postgres_properties = {\"user\": \"exemploUsuario\", \"password\": \"exemploSenha\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Criação do schema para o DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"valor\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Dados para inserir no banco de dados\n",
    "data = [(2, 'Cliente2', 100.0)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Escrevendo o DataFrame no banco de dados, criando a tabela\n",
    "df.write.jdbc(url=postgres_url, table=\"public.clientes\", mode=\"append\", properties=postgres_properties)\n",
    "\n",
    "# Lendo e exibindo os dados\n",
    "df_postgres = spark.read.jdbc(url=postgres_url, table=\"public.clientes\", properties=postgres_properties)\n",
    "df_postgres.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "126dfd1d-36ad-42b6-b9a1-c87ef7ecd0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conexão com o PostgreSQL em outro contêiner Docker\n",
    "postgres_url = \"jdbc:postgresql://postgres:5432/exemploDB\"\n",
    "postgres_properties = {\"user\": \"exemploUsuario\", \"password\": \"exemploSenha\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Definir o caminho do arquivo CSV\n",
    "csv_file_path = 'arquivos/teste.csv'\n",
    "\n",
    "# Ler os dados do CSV para um DataFrame. Aqui, assumimos que o arquivo CSV possui um cabeçalho.\n",
    "# Também inferimos automaticamente o esquema dos dados. Adicionalmente, como o delimitador\n",
    "# padrão de um arquivo CSV é uma vírgula e o arquivo que estamos usando pode ter um delimitador diferente\n",
    "# (como ponto e vírgula), especificamos isso na opção 'sep'. Também indicamos o caractere de aspas\n",
    "# com a opção 'quote'. Para lidar com questões de codificação de caracteres que podem surgir ao trabalhar\n",
    "# com arquivos em formatos diferentes de UTF-8, usamos a opção 'encoding' para definir a codificação\n",
    "# correta, que neste caso é 'ISO-8859-1'.\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .csv(csv_file_path)\n",
    "\n",
    "# Escrevendo o DataFrame no banco de dados, criando a tabela\n",
    "df.write.jdbc(url=postgres_url, table=\"public.clientes\", mode=\"append\", properties=postgres_properties)\n",
    "\n",
    "# Lendo e exibindo os dados\n",
    "df_postgres = spark.read.jdbc(url=postgres_url, table=\"public.clientes\", properties=postgres_properties)\n",
    "df_postgres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e473b653-fd6e-42bf-8888-c0cb59f4f9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos os registros foram deletados com sucesso da tabela clientes.\n",
      "Conexão com PostgreSQL fechada\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Estabeleça a conexão com o banco de dados PostgreSQL\n",
    "try:\n",
    "    # Conecte-se ao banco de dados PostgreSQL\n",
    "    connection = psycopg2.connect(\n",
    "        user=\"exemploUsuario\",\n",
    "        password=\"exemploSenha\",\n",
    "        host=\"postgres\",\n",
    "        port=\"5432\",  # A porta mapeada para o contêiner Docker\n",
    "        database=\"exemploDB\"\n",
    "    )\n",
    "\n",
    "    # Crie um cursor para executar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Comando SQL para deletar todos os registros da tabela\n",
    "    delete_query = \"DELETE FROM public.clientes\"\n",
    "\n",
    "    # Execute o comando SQL\n",
    "    cursor.execute(delete_query)\n",
    "    connection.commit()\n",
    "    print(\"Todos os registros foram deletados com sucesso da tabela clientes.\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Erro ao conectar ao PostgreSQL\", error)\n",
    "\n",
    "finally:\n",
    "    # Fechar a conexão e o cursor\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Conexão com PostgreSQL fechada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b3f4910-31a0-4952-a54a-a9da4a1d68a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os registros foram atualizados com sucesso.\n",
      "A conexão com o PostgreSQL foi fechada.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicialização do SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Carregar o DataFrame a partir de um arquivo CSV\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"arquivos/teste_update.csv\")\n",
    "\n",
    "# Converter o DataFrame em um RDD e coletar os dados para a memória local\n",
    "records = df.rdd.map(lambda row: (row['id'], row['nome'], row['valor'])).collect()\n",
    "\n",
    "# Conectar ao banco de dados PostgreSQL e executar o update linha por linha\n",
    "try:\n",
    "    connection = psycopg2.connect(user=\"exemploUsuario\",\n",
    "                                  password=\"exemploSenha\",\n",
    "                                  host=\"postgres\",\n",
    "                                  port=\"5432\",\n",
    "                                  database=\"exemploDB\")\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    update_query = \"UPDATE public.clientes SET nome = %s, valor = %s WHERE id = %s\"\n",
    "    \n",
    "    for record in records:\n",
    "        cursor.execute(update_query, (record[1], record[2], record[0]))\n",
    "    \n",
    "    connection.commit()\n",
    "    print(\"Os registros foram atualizados com sucesso.\")\n",
    "    \n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Erro ao atualizar os registros: \", error)\n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"A conexão com o PostgreSQL foi fechada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9b12162-18ff-484b-ab45-cb7d5dc90c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "| id|    nome|  valor|\n",
      "+---+--------+-------+\n",
      "|  1|Cliente1| 200.78|\n",
      "|  2|Cliente2| 500.09|\n",
      "|  3|Cliente3| 1100.1|\n",
      "|  4|Cliente4|1900.56|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo e exibindo os dados\n",
    "df_postgres = spark.read.jdbc(url=postgres_url, table=\"public.clientes\", properties=postgres_properties)\n",
    "df_postgres.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3ff30-f18e-45d5-8ee3-f9f428b9617d",
   "metadata": {},
   "source": [
    "## SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d012eb-7b4a-4694-bb44-c39ef5944aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Caminho para os drivers JDBC dentro do contêiner Docker\n",
    "path_sqlserver = \"/opt/spark/jars/sqljdbc42.jar\"\n",
    "\n",
    "# Caminho para o arquivo CSV\n",
    "csv_file_path = 'arquivos/teste.csv'\n",
    "\n",
    "# Configurando a SparkSession para incluir os drivers JDBC\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars\", f\"{path_sqlserver}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ler os dados do CSV para um DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# Propriedades de conexão com o SQL Server\n",
    "sqlserver_url = \"jdbc:sqlserver://sqlserver:1433;databaseName=tempdb\"\n",
    "sqlserver_properties = {\n",
    "    \"user\": \"SA\",\n",
    "    \"password\": \"exemploSenha1!\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Gravar o DataFrame no SQL Server\n",
    "df.write.jdbc(url=sqlserver_url, table=\"dbo.clientes\", mode=\"append\", properties=sqlserver_properties)\n",
    "\n",
    "# Lendo e exibindo os dados para verificar a inserção\n",
    "df_sqlserver = spark.read.jdbc(url=sqlserver_url, table=\"dbo.clientes\", properties=sqlserver_properties)\n",
    "df_sqlserver.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46b7d4-66ab-43aa-bfed-01ea13d5aa44",
   "metadata": {},
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6447b2-ccc1-4519-9f09-d5d8c2fab3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Caminho para os drivers JDBC dentro do contêiner Docker\n",
    "path_mysql = \"/opt/spark/jars/mysql-connector-java-8.0.29.jar\"\n",
    "\n",
    "# Caminho para o arquivo CSV\n",
    "csv_file_path = 'arquivos/teste.csv'\n",
    "\n",
    "# Configurando a SparkSession para incluir os drivers JDBC\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars\", f\"{path_mysql}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ler os dados do CSV para um DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# Propriedades de conexão com o MySQL\n",
    "mysql_url = \"jdbc:mysql://mysql:3306/exemploDB\"\n",
    "mysql_properties = {\n",
    "    \"user\": \"exemploUsuario\",\n",
    "    \"password\": \"exemploSenhaUsuario\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Gravar o DataFrame no MySQL\n",
    "df.write.jdbc(url=mysql_url, table=\"clientes\", mode=\"append\", properties=mysql_properties)\n",
    "\n",
    "# Lendo e exibindo os dados para verificar a inserção\n",
    "df_mysql = spark.read.jdbc(url=mysql_url, table=\"clientes\", properties=mysql_properties)\n",
    "df_mysql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c3c06-3c67-4f37-a2cb-d3f8c3ce687a",
   "metadata": {},
   "source": [
    "## Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd8ebc2-ec62-4812-9a0b-6b23a39ef737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do Spark: 3.5.0\n",
      "Versão do Delta Lake: 3.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "\n",
    "# Imprime a versão do Spark\n",
    "print(\"Versão do Spark:\", pyspark.__version__)\n",
    "\n",
    "# Tenta imprimir a versão do Delta Lake\n",
    "try:\n",
    "    print(\"Versão do Delta Lake:\", version(\"delta-spark\"))\n",
    "except PackageNotFoundError:\n",
    "    print(\"Delta Lake não está instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c0d29a-c860-4a34-b0ad-4c1e068a5751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a343b4c1-88ce-4f9e-b5c4-66c74ef5d60c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df = spark.read.csv('arquivos/teste.csv', header=True, inferSchema=True)\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"arquivos/tmp/delta/teste\")\n",
    "\n",
    "# Ler dados no formato Delta\n",
    "delta_df = spark.read.format(\"delta\").load(\"arquivos/tmp/delta/teste\")\n",
    "delta_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9054c7c2-bd31-4d65-aa44-f446f8ed9339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versionAsOf: 0\n",
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n",
      "versionAsOf: 1\n",
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|150.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|200.0|\n",
      "|  4|Cliente4|100.0|\n",
      "|  5|Cliente5|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Acessando uma versão específica de um conjunto de dados\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"arquivos/tmp/delta/teste\")\n",
    "print(\"versionAsOf: 0\")\n",
    "df.show()\n",
    "\n",
    "# Acessando uma versão específica de um conjunto de dados\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"arquivos/tmp/delta/teste\")\n",
    "print(\"versionAsOf: 1\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28aebb90-5041-4aba-9225-62a7a1152dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01 14:10:54\").load(\"arquivos/tmp/delta/teste\")\n",
    "df_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209133c8-0e63-4029-9d24-1478218dd765",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|     12|2024-01-01 17:17:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|         11|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.5....|\n",
      "|     11|2024-01-01 15:46:...|  NULL|    NULL|    MERGE|{predicate -> (CA...|NULL|    NULL|     NULL|         10|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.3....|\n",
      "|     10|2024-01-01 15:45:...|  NULL|    NULL|   UPDATE|{predicate -> (id...|NULL|    NULL|     NULL|          9|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.3....|\n",
      "|      9|2024-01-01 15:45:...|  NULL|    NULL|    MERGE|{predicate -> (CA...|NULL|    NULL|     NULL|          8|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.3....|\n",
      "|      8|2024-01-01 15:45:...|  NULL|    NULL|   UPDATE|{predicate -> (id...|NULL|    NULL|     NULL|          7|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.3....|\n",
      "|      7|2024-01-01 15:30:...|  NULL|    NULL|    MERGE|{predicate -> (CA...|NULL|    NULL|     NULL|          6|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.3....|\n",
      "|      6|2024-01-01 15:30:...|  NULL|    NULL|   UPDATE|{predicate -> (id...|NULL|    NULL|     NULL|          5|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.3....|\n",
      "|      5|2024-01-01 15:30:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          4|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.3....|\n",
      "|      4|2024-01-01 15:28:...|  NULL|    NULL|    MERGE|{predicate -> (CA...|NULL|    NULL|     NULL|          3|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/3.3....|\n",
      "|      3|2024-01-01 15:28:...|  NULL|    NULL|   UPDATE|{predicate -> (id...|NULL|    NULL|     NULL|          2|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.3....|\n",
      "|      2|2024-01-01 14:48:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          1|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.3....|\n",
      "|      1|2024-01-01 14:37:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.3....|\n",
      "|      0|2024-01-01 14:10:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"arquivos/tmp/delta/teste\")\n",
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2eed3c5-825a-4de2-a9ac-2a3e332e4008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# Carregar a tabela Delta\n",
    "deltaTable = DeltaTable.forPath(spark, \"arquivos/tmp/delta/teste\")\n",
    "\n",
    "# Definir o período de retenção para 6 meses (em horas)\n",
    "RETENTION_HOURS = 6 * 30 * 24  # 6 meses em horas\n",
    "\n",
    "# Executar o comando VACUUM para remover versões antigas dos dados\n",
    "# ATENÇÃO: O VACUUM é uma operação destrutiva e não pode ser desfeita.\n",
    "deltaTable.vacuum(RETENTION_HOURS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd369126-1811-4f79-a8f0-22e94818311b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|150.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n",
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|150.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|200.0|\n",
      "|  4|Cliente4|100.0|\n",
      "|  5|Cliente5|100.0|\n",
      "+---+--------+-----+\n",
      "\n",
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|150.0|\n",
      "|  3|Cliente3|200.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Inicialize sua SparkSession aqui\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeExample\").getOrCreate()\n",
    "\n",
    "# Operações com a tabela Delta\n",
    "deltaTable = DeltaTable.forPath(spark, \"arquivos/tmp/delta/teste\")\n",
    "\n",
    "# Update\n",
    "deltaTable.update(\n",
    "    condition=expr(\"id == 1\"),\n",
    "    set={\"valor\": expr(\"150.0\")}\n",
    ")\n",
    "\n",
    "# Leitura para exibição dos resultados\n",
    "delta_df = spark.read.format(\"delta\").load(\"arquivos/tmp/delta/teste\")\n",
    "delta_df.show()\n",
    "\n",
    "# Dados para merge\n",
    "updatesDF = spark.createDataFrame([(3, \"Cliente3\", 200.0), (5, \"Cliente5\", 100.0)], [\"id\", \"nome\", \"valor\"])\n",
    "\n",
    "# Merge\n",
    "deltaTable.alias(\"tabela\").merge(\n",
    "    updatesDF.alias(\"updates\"),\n",
    "    \"tabela.id = updates.id\"\n",
    ").whenMatchedUpdate(set={\"valor\": \"updates.valor\"}) \\\n",
    " .whenNotMatchedInsert(values={\"id\": \"updates.id\", \"nome\": \"updates.nome\", \"valor\": \"updates.valor\"}) \\\n",
    " .execute()\n",
    "\n",
    "# Leitura para exibição dos resultados\n",
    "delta_df = spark.read.format(\"delta\").load(\"arquivos/tmp/delta/teste\")\n",
    "delta_df.show()\n",
    "\n",
    "# Delete\n",
    "deltaTable.delete(condition=expr(\"valor < 150.0\"))\n",
    "\n",
    "# Leitura final para exibição dos resultados\n",
    "delta_df = spark.read.format(\"delta\").load(\"arquivos/tmp/delta/teste\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fec73-1dfc-4d71-905d-fdd0e0bfd10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

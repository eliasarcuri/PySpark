{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea3bae-cc33-425b-8da2-4d98746158c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Caminho para os drivers JDBC dentro do contêiner Docker\n",
    "path_postgresql = \"/opt/spark/jars/postgresql-42.2.5.jar\"\n",
    "path_mysql = \"/opt/spark/jars/mysql-connector-java-8.0.29.jar\"\n",
    "path_sqlserver = \"/opt/spark/jars/sqljdbc42.jar\"\n",
    "\n",
    "# Configurando a SparkSession para incluir os drivers JDBC\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars\", f\"{path_postgresql},{path_mysql},{path_sqlserver}\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e97421-dd2f-44d8-9865-18a995816c1f",
   "metadata": {},
   "source": [
    "## Criar schema se precisar criar a tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a14d3f-fdd0-4c20-9230-fb28dfd07057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "| id|      nome|valor|\n",
      "+---+----------+-----+\n",
      "|  1|  Cliente1|100.0|\n",
      "|  1|  Cliente1|100.0|\n",
      "|  1|  Cliente1|100.0|\n",
      "|  1|  Cliente1|100.0|\n",
      "|  1|'Cliente1'|100.0|\n",
      "|  2|  Cliente2|100.0|\n",
      "+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conexão com o PostgreSQL em outro contêiner Docker\n",
    "postgres_url = \"jdbc:postgresql://postgres:5432/exemploDB\"\n",
    "postgres_properties = {\"user\": \"exemploUsuario\", \"password\": \"exemploSenha\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Criação do schema para o DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"valor\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Dados para inserir no banco de dados\n",
    "data = [(2, 'Cliente2', 100.0)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Escrevendo o DataFrame no banco de dados, criando a tabela\n",
    "df.write.jdbc(url=postgres_url, table=\"public.clientes\", mode=\"append\", properties=postgres_properties)\n",
    "\n",
    "# Lendo e exibindo os dados\n",
    "df_postgres = spark.read.jdbc(url=postgres_url, table=\"public.clientes\", properties=postgres_properties)\n",
    "df_postgres.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "126dfd1d-36ad-42b6-b9a1-c87ef7ecd0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conexão com o PostgreSQL em outro contêiner Docker\n",
    "postgres_url = \"jdbc:postgresql://postgres:5432/exemploDB\"\n",
    "postgres_properties = {\"user\": \"exemploUsuario\", \"password\": \"exemploSenha\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Definir o caminho do arquivo CSV\n",
    "csv_file_path = 'arquivos/teste.csv'\n",
    "\n",
    "# Ler os dados do CSV para um DataFrame. Aqui, assumimos que o arquivo CSV possui um cabeçalho.\n",
    "# Também inferimos automaticamente o esquema dos dados. Adicionalmente, como o delimitador\n",
    "# padrão de um arquivo CSV é uma vírgula e o arquivo que estamos usando pode ter um delimitador diferente\n",
    "# (como ponto e vírgula), especificamos isso na opção 'sep'. Também indicamos o caractere de aspas\n",
    "# com a opção 'quote'. Para lidar com questões de codificação de caracteres que podem surgir ao trabalhar\n",
    "# com arquivos em formatos diferentes de UTF-8, usamos a opção 'encoding' para definir a codificação\n",
    "# correta, que neste caso é 'ISO-8859-1'.\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .csv(csv_file_path)\n",
    "\n",
    "# Escrevendo o DataFrame no banco de dados, criando a tabela\n",
    "df.write.jdbc(url=postgres_url, table=\"public.clientes\", mode=\"append\", properties=postgres_properties)\n",
    "\n",
    "# Lendo e exibindo os dados\n",
    "df_postgres = spark.read.jdbc(url=postgres_url, table=\"public.clientes\", properties=postgres_properties)\n",
    "df_postgres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e473b653-fd6e-42bf-8888-c0cb59f4f9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos os registros foram deletados com sucesso da tabela clientes.\n",
      "Conexão com PostgreSQL fechada\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Estabeleça a conexão com o banco de dados PostgreSQL\n",
    "try:\n",
    "    # Conecte-se ao banco de dados PostgreSQL\n",
    "    connection = psycopg2.connect(\n",
    "        user=\"exemploUsuario\",\n",
    "        password=\"exemploSenha\",\n",
    "        host=\"postgres\",\n",
    "        port=\"5432\",  # A porta mapeada para o contêiner Docker\n",
    "        database=\"exemploDB\"\n",
    "    )\n",
    "\n",
    "    # Crie um cursor para executar comandos SQL\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Comando SQL para deletar todos os registros da tabela\n",
    "    delete_query = \"DELETE FROM public.clientes\"\n",
    "\n",
    "    # Execute o comando SQL\n",
    "    cursor.execute(delete_query)\n",
    "    connection.commit()\n",
    "    print(\"Todos os registros foram deletados com sucesso da tabela clientes.\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Erro ao conectar ao PostgreSQL\", error)\n",
    "\n",
    "finally:\n",
    "    # Fechar a conexão e o cursor\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Conexão com PostgreSQL fechada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b3f4910-31a0-4952-a54a-a9da4a1d68a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os registros foram atualizados com sucesso.\n",
      "A conexão com o PostgreSQL foi fechada.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicialização do SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Carregar o DataFrame a partir de um arquivo CSV\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"arquivos/teste_update.csv\")\n",
    "\n",
    "# Converter o DataFrame em um RDD e coletar os dados para a memória local\n",
    "records = df.rdd.map(lambda row: (row['id'], row['nome'], row['valor'])).collect()\n",
    "\n",
    "# Conectar ao banco de dados PostgreSQL e executar o update linha por linha\n",
    "try:\n",
    "    connection = psycopg2.connect(user=\"exemploUsuario\",\n",
    "                                  password=\"exemploSenha\",\n",
    "                                  host=\"postgres\",\n",
    "                                  port=\"5432\",\n",
    "                                  database=\"exemploDB\")\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    update_query = \"UPDATE public.clientes SET nome = %s, valor = %s WHERE id = %s\"\n",
    "    \n",
    "    for record in records:\n",
    "        cursor.execute(update_query, (record[1], record[2], record[0]))\n",
    "    \n",
    "    connection.commit()\n",
    "    print(\"Os registros foram atualizados com sucesso.\")\n",
    "    \n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Erro ao atualizar os registros: \", error)\n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"A conexão com o PostgreSQL foi fechada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9b12162-18ff-484b-ab45-cb7d5dc90c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "| id|    nome|  valor|\n",
      "+---+--------+-------+\n",
      "|  1|Cliente1| 200.78|\n",
      "|  2|Cliente2| 500.09|\n",
      "|  3|Cliente3| 1100.1|\n",
      "|  4|Cliente4|1900.56|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo e exibindo os dados\n",
    "df_postgres = spark.read.jdbc(url=postgres_url, table=\"public.clientes\", properties=postgres_properties)\n",
    "df_postgres.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3ff30-f18e-45d5-8ee3-f9f428b9617d",
   "metadata": {},
   "source": [
    "## SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d012eb-7b4a-4694-bb44-c39ef5944aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Caminho para os drivers JDBC dentro do contêiner Docker\n",
    "path_sqlserver = \"/opt/spark/jars/sqljdbc42.jar\"\n",
    "\n",
    "# Caminho para o arquivo CSV\n",
    "csv_file_path = 'arquivos/teste.csv'\n",
    "\n",
    "# Configurando a SparkSession para incluir os drivers JDBC\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars\", f\"{path_sqlserver}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ler os dados do CSV para um DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# Propriedades de conexão com o SQL Server\n",
    "sqlserver_url = \"jdbc:sqlserver://sqlserver:1433;databaseName=tempdb\"\n",
    "sqlserver_properties = {\n",
    "    \"user\": \"SA\",\n",
    "    \"password\": \"exemploSenha1!\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Gravar o DataFrame no SQL Server\n",
    "df.write.jdbc(url=sqlserver_url, table=\"dbo.clientes\", mode=\"append\", properties=sqlserver_properties)\n",
    "\n",
    "# Lendo e exibindo os dados para verificar a inserção\n",
    "df_sqlserver = spark.read.jdbc(url=sqlserver_url, table=\"dbo.clientes\", properties=sqlserver_properties)\n",
    "df_sqlserver.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46b7d4-66ab-43aa-bfed-01ea13d5aa44",
   "metadata": {},
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6447b2-ccc1-4519-9f09-d5d8c2fab3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Caminho para os drivers JDBC dentro do contêiner Docker\n",
    "path_mysql = \"/opt/spark/jars/mysql-connector-java-8.0.29.jar\"\n",
    "\n",
    "# Caminho para o arquivo CSV\n",
    "csv_file_path = 'arquivos/teste.csv'\n",
    "\n",
    "# Configurando a SparkSession para incluir os drivers JDBC\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars\", f\"{path_mysql}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ler os dados do CSV para um DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# Propriedades de conexão com o MySQL\n",
    "mysql_url = \"jdbc:mysql://mysql:3306/exemploDB\"\n",
    "mysql_properties = {\n",
    "    \"user\": \"exemploUsuario\",\n",
    "    \"password\": \"exemploSenhaUsuario\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Gravar o DataFrame no MySQL\n",
    "df.write.jdbc(url=mysql_url, table=\"clientes\", mode=\"append\", properties=mysql_properties)\n",
    "\n",
    "# Lendo e exibindo os dados para verificar a inserção\n",
    "df_mysql = spark.read.jdbc(url=mysql_url, table=\"clientes\", properties=mysql_properties)\n",
    "df_mysql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c3c06-3c67-4f37-a2cb-d3f8c3ce687a",
   "metadata": {},
   "source": [
    "## Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a343b4c1-88ce-4f9e-b5c4-66c74ef5d60c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Ler dados do arquivo CSV\n",
    "df = spark.read.csv('arquivos/teste.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Escrever dados no formato Delta\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"arquivos/tmp/delta/teste\")\n",
    "\n",
    "# Ler dados no formato Delta\n",
    "delta_df = spark.read.format(\"delta\").load(\"arquivos/tmp/delta/teste\")\n",
    "delta_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9054c7c2-bd31-4d65-aa44-f446f8ed9339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versionAsOf: 0\n",
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n",
      "versionAsOf: 1\n",
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "|  5|Cliente5|200.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Acessando uma versão específica de um conjunto de dados\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"arquivos/tmp/delta/teste\")\n",
    "print(\"versionAsOf: 0\")\n",
    "df.show()\n",
    "\n",
    "# Acessando uma versão específica de um conjunto de dados\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"arquivos/tmp/delta/teste\")\n",
    "print(\"versionAsOf: 1\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28aebb90-5041-4aba-9225-62a7a1152dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|100.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|100.0|\n",
      "|  4|Cliente4|100.0|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01 14:10:54\").load(\"arquivos/tmp/delta/teste\")\n",
    "df_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "209133c8-0e63-4029-9d24-1478218dd765",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2024-01-01 14:48:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|  Serializable|        false|{numFiles -> 1, n...|        null|Apache-Spark/3.3....|\n",
      "|      1|2024-01-01 14:37:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          0|  Serializable|        false|{numFiles -> 1, n...|        null|Apache-Spark/3.3....|\n",
      "|      0|2024-01-01 14:10:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 1, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"arquivos/tmp/delta/teste\")\n",
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2eed3c5-825a-4de2-a9ac-2a3e332e4008",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# Carregar a tabela Delta\n",
    "deltaTable = DeltaTable.forPath(spark, \"arquivos/tmp/delta/teste\")\n",
    "\n",
    "# Definir o período de retenção para 6 meses (em horas)\n",
    "RETENTION_HOURS = 6 * 30 * 24  # 6 meses em horas\n",
    "\n",
    "# Executar o comando VACUUM para remover versões antigas dos dados\n",
    "# ATENÇÃO: O VACUUM é uma operação destrutiva e não pode ser desfeita.\n",
    "deltaTable.vacuum(RETENTION_HOURS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd369126-1811-4f79-a8f0-22e94818311b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|150.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|200.0|\n",
      "|  4|Cliente4|100.0|\n",
      "|  5|Cliente5|100.0|\n",
      "+---+--------+-----+\n",
      "\n",
      "+---+--------+-----+\n",
      "| id|    nome|valor|\n",
      "+---+--------+-----+\n",
      "|  1|Cliente1|150.0|\n",
      "|  2|Cliente2|100.0|\n",
      "|  3|Cliente3|200.0|\n",
      "|  4|Cliente4|100.0|\n",
      "|  5|Cliente5|100.0|\n",
      "+---+--------+-----+\n",
      "\n",
      "Versão do Spark: 3.3.2\n"
     ]
    },
    {
     "ename": "ContextualVersionConflict",
     "evalue": "(pyspark 3.3.2 (/usr/local/spark-3.3.2-bin-hadoop3/python), Requirement.parse('pyspark<3.6.0,>=3.5.0'), {'delta-spark'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Imprime a versão do Delta Lake, se instalada\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersão do Delta Lake:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpkg_resources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta-spark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pkg_resources\u001b[38;5;241m.\u001b[39mDistributionNotFound:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelta Lake não está instalado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:526\u001b[0m, in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    524\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Requirement\u001b[38;5;241m.\u001b[39mparse(dist)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Requirement):\n\u001b[0;32m--> 526\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mget_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Distribution):\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string, Requirement, or Distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:398\u001b[0m, in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m working_set\u001b[38;5;241m.\u001b[39mfind(moduleOrReq) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mrequire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmoduleOrReq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m     module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[moduleOrReq]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:966\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mrequirements):\n\u001b[1;32m    958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \n\u001b[1;32m    960\u001b[0m \u001b[38;5;124;03m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;124;03m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 966\u001b[0m     needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_requirements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m needed:\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(dist)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:827\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m req_extras\u001b[38;5;241m.\u001b[39mmarkers_pass(req, extras):\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_dist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace_conflicting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstaller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequired_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_activate\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# push the new requirements onto the stack\u001b[39;00m\n\u001b[1;32m    832\u001b[0m new_requirements \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrequires(req\u001b[38;5;241m.\u001b[39mextras)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:873\u001b[0m, in \u001b[0;36mWorkingSet._resolve_dist\u001b[0;34m(self, req, best, replace_conflicting, env, installer, required_by, to_activate)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m req:\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;66;03m# Oops, the \"best\" so far conflicts with a dependency\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     dependent_req \u001b[38;5;241m=\u001b[39m required_by[req]\n\u001b[0;32m--> 873\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VersionConflict(dist, req)\u001b[38;5;241m.\u001b[39mwith_context(dependent_req)\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m: (pyspark 3.3.2 (/usr/local/spark-3.3.2-bin-hadoop3/python), Requirement.parse('pyspark<3.6.0,>=3.5.0'), {'delta-spark'})"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"arquivos/tmp/delta/teste\")\n",
    "deltaTable.update(\n",
    "   condition = expr(\"id == 1\"),\n",
    "   set = {\"valor\": expr(\"150.0\")}\n",
    ")\n",
    "\n",
    "delta_df = spark.read.format(\"delta\").load(\"arquivos/tmp/delta/teste\")\n",
    "delta_df.show()\n",
    "\n",
    "updatesDF = spark.createDataFrame([(3, \"Cliente3\", 200.0), (5, \"Cliente5\", 100.0)], [\"id\", \"nome\", \"valor\"])\n",
    "deltaTable = DeltaTable.forPath(spark, \"arquivos/tmp/delta/teste\")\n",
    "\n",
    "deltaTable.alias(\"tabela\").merge(\n",
    "   updatesDF.alias(\"updates\"),\n",
    "   \"tabela.id = updates.id\"\n",
    ").whenMatchedUpdate(set = {\"valor\": \"updates.valor\"}) \\\n",
    " .whenNotMatchedInsert(values = {\"id\": \"updates.id\", \"nome\": \"updates.nome\", \"valor\": \"updates.valor\"}) \\\n",
    " .execute()\n",
    "\n",
    "delta_df = spark.read.format(\"delta\").load(\"arquivos/tmp/delta/teste\")\n",
    "delta_df.show()\n",
    "\n",
    "import pyspark\n",
    "import pkg_resources\n",
    "\n",
    "# Imprime a versão do Spark\n",
    "print(\"Versão do Spark:\", pyspark.__version__)\n",
    "\n",
    "# Imprime a versão do Delta Lake, se instalada\n",
    "try:\n",
    "    print(\"Versão do Delta Lake:\", pkg_resources.get_distribution(\"delta-spark\").version)\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    print(\"Delta Lake não está instalado.\")\n",
    "\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"arquivos/tmp/delta/teste\")\n",
    "deltaTable.delete(condition = expr(\"valor < 150.0\"))\n",
    "\n",
    "delta_df = spark.read.format(\"delta\").load(\"arquivos/tmp/delta/teste\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fec73-1dfc-4d71-905d-fdd0e0bfd10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb37cb5c-7001-4820-bf8e-d78fe179208d",
   "metadata": {},
   "source": [
    "## Carregando e Explorando o Conjunto de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8976e2-5c6c-4f34-93e3-501dcd664f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.name =  ProductDataAnalysis\n",
      "spark.master =  local[2]\n",
      "spark.executor.memory =  1g\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Create a SparkConf object\n",
    "conf = SparkConf().setAppName(\"ProductDataAnalysis\") \\\n",
    "           .setMaster(\"local[2]\") \\\n",
    "           .set(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "# Create a SparkSession object\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Retrieve the SparkConf object from the SparkContext\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "# Print the configuration settings\n",
    "print(\"spark.app.name = \", conf.get(\"spark.app.name\"))\n",
    "print(\"spark.master = \", conf.get(\"spark.master\"))\n",
    "print(\"spark.executor.memory = \", conf.get(\"spark.executor.memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9a44ef-3866-465e-8124-a59751a885aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carregue os dados do arquivo CSV que já está dentro do seu projeto\n",
    "file_path = \"product+classification+and+clustering/pricerunner_aggregate.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1d2e0e-5c39-4adc-b6b5-15770a8134b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+-----------+--------------------+------------+---------------+\n",
      "|Product ID|       Product Title| Merchant ID| Cluster ID|       Cluster Label| Category ID| Category Label|\n",
      "+----------+--------------------+------------+-----------+--------------------+------------+---------------+\n",
      "|         1|apple iphone 8 pl...|           1|          1|Apple iPhone 8 Pl...|        2612|  Mobile Phones|\n",
      "|         2|apple iphone 8 pl...|           2|          1|Apple iPhone 8 Pl...|        2612|  Mobile Phones|\n",
      "|         3|apple mq8n2b/a ip...|           3|          1|Apple iPhone 8 Pl...|        2612|  Mobile Phones|\n",
      "|         4|apple iphone 8 pl...|           4|          1|Apple iPhone 8 Pl...|        2612|  Mobile Phones|\n",
      "|         5|apple iphone 8 pl...|           5|          1|Apple iPhone 8 Pl...|        2612|  Mobile Phones|\n",
      "+----------+--------------------+------------+-----------+--------------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a852d7-af0e-4cb9-906f-ca9e383a1dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product ID: integer (nullable = true)\n",
      " |-- Product Title: string (nullable = true)\n",
      " |--  Merchant ID: integer (nullable = true)\n",
      " |--  Cluster ID: integer (nullable = true)\n",
      " |--  Cluster Label: string (nullable = true)\n",
      " |--  Category ID: integer (nullable = true)\n",
      " |--  Category Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c51933-62ce-4493-bd88-9f53f6b1ae7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------------+------------------+--------------------+------------------+----------------+\n",
      "|summary|       Product ID|       Product Title|       Merchant ID|        Cluster ID|       Cluster Label|       Category ID|  Category Label|\n",
      "+-------+-----------------+--------------------+------------------+------------------+--------------------+------------------+----------------+\n",
      "|  count|            35311|               35311|             35311|             35311|               35311|             35311|           35311|\n",
      "|   mean|26150.80017558268|                null|120.50188326583785|30110.687632749003|                null| 2618.142929965167|            null|\n",
      "| stddev|13498.19122018199|                null|117.04555721851422|18410.265642128295|                null|3.6007080727956833|            null|\n",
      "|    min|                1|10 hd portable te...|                 1|                 1|AEG A71101TSX0 St...|              2612|            CPUs|\n",
      "|    max|            47358|zwf91283w 9kg 120...|               371|             47525|  iceQ Iceq48b Black|              2623|Washing Machines|\n",
      "+-------+-----------------+--------------------+------------------+------------------+--------------------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ae151f-1202-47a0-ae4e-ac687db11ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.extraJavaOptions', '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'), ('spark.app.submitTime', '1703852316041'), ('spark.app.id', 'local-1703852317194'), ('spark.driver.host', 'f58eefab1b97'), ('spark.executor.id', 'driver'), ('spark.app.startTime', '1703852316193'), ('spark.app.name', 'ProductDataAnalysis'), ('spark.executor.memory', '1g'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'), ('spark.sql.warehouse.dir', 'file:/home/app/app/spark-warehouse'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.driver.port', '45303'), ('spark.ui.showConsoleProgress', 'true'), ('spark.master', 'local[2]')]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f20186-b9d0-4977-99b5-04b9fd1aa20a",
   "metadata": {},
   "source": [
    "## Limpeza e Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b949d0ed-0546-4a1a-b34c-ebb489b1fa23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------+-----------+--------------+------------+---------------+\n",
      "|Product ID|Product Title| Merchant ID| Cluster ID| Cluster Label| Category ID| Category Label|\n",
      "+----------+-------------+------------+-----------+--------------+------------+---------------+\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "|     false|        false|       false|      false|         false|       false|          false|\n",
      "+----------+-------------+------------+-----------+--------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select([col(c).isNull().alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9114ec3a-9383-4c00-a1c2-e2a0749fef43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exemplo de exclusão de linhas com valores nulos\n",
    "df_clean = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16358bf-0ff7-43c7-beb0-8d9200c286e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product ID: integer (nullable = true)\n",
      " |-- Product Title: string (nullable = true)\n",
      " |--  Merchant ID: integer (nullable = true)\n",
      " |--  Cluster ID: integer (nullable = true)\n",
      " |--  Cluster Label: string (nullable = true)\n",
      " |--  Category ID: integer (nullable = true)\n",
      " |--  Category Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b90ad7-173c-4852-bd3c-3af49f558299",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product ID: integer (nullable = true)\n",
      " |-- Product Title: string (nullable = true)\n",
      " |-- Merchant ID: integer (nullable = true)\n",
      " |-- Cluster ID: integer (nullable = true)\n",
      " |-- Cluster Label: string (nullable = true)\n",
      " |-- Category ID: integer (nullable = true)\n",
      " |-- Category Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de nomes de colunas conforme o esquema com espaços\n",
    "col_names = df_clean.schema.names\n",
    "\n",
    "# Renomear colunas para remover espaços iniciais\n",
    "for col_name in col_names:\n",
    "    new_col_name = col_name.strip()  # strip() remove espaços do começo e do fim\n",
    "    df_clean = df_clean.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# Verificando o novo esquema\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c25528c-d80c-4990-bb0d-e2dac55d81ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exemplo de conversão de uma coluna de string para inteiro\n",
    "df_clean = df_clean.withColumn(\"Merchant ID\", df_clean[\"Merchant ID\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848f6568-92c0-41cf-8772-df6ffc268247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product ID: integer (nullable = true)\n",
      " |-- Product Title: string (nullable = true)\n",
      " |-- Merchant ID: integer (nullable = true)\n",
      " |-- Cluster ID: integer (nullable = true)\n",
      " |-- Cluster Label: string (nullable = true)\n",
      " |-- Category ID: integer (nullable = true)\n",
      " |-- Category Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTA: A conversão de tipos de dados é útil quando você precisa alterar o tipo de uma coluna. \n",
    "# No nosso DataFrame, a coluna 'Merchant ID' já é do tipo inteiro, conforme mostrado pelo esquema:\n",
    "df_clean.printSchema()\n",
    "\n",
    "# Portanto, a conversão para inteiro não é necessária neste caso. Se fosse uma coluna do tipo string\n",
    "# que contém apenas números, a conversão seria realizada da seguinte maneira:\n",
    "# df_clean = df_clean.withColumn(\"Merchant ID\", df_clean[\"Merchant ID\"].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce35fd1-c46f-40b8-9249-51229240a83d",
   "metadata": {},
   "source": [
    "## Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21211097-7a51-43f5-bdb9-ed3c6d379dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|  Category Label|Count|\n",
      "+----------------+-----+\n",
      "| Fridge Freezers| 5501|\n",
      "|   Mobile Phones| 4081|\n",
      "|Washing Machines| 4044|\n",
      "|            CPUs| 3862|\n",
      "|         Fridges| 3584|\n",
      "|             TVs| 3564|\n",
      "|     Dishwashers| 3424|\n",
      "| Digital Cameras| 2697|\n",
      "|      Microwaves| 2342|\n",
      "|        Freezers| 2212|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculando a Distribuição de Produtos por Categoria\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Agrupando por categoria e contando os produtos\n",
    "categoria_distribuicao = df_clean.groupBy(\"Category Label\").agg(count(\"Product ID\").alias(\"Count\")).orderBy(\"Count\", ascending=False)\n",
    "\n",
    "# Visualizando o resultado\n",
    "categoria_distribuicao.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f233ed-d1b8-458d-b5e2-9356469bcc25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Merchant ID|Total Products|\n",
      "+-----------+--------------+\n",
      "|          3|          2547|\n",
      "|          6|          1591|\n",
      "|        298|          1523|\n",
      "|         31|          1350|\n",
      "|        119|          1239|\n",
      "|          7|          1204|\n",
      "|         17|          1193|\n",
      "|        293|          1177|\n",
      "|        294|          1000|\n",
      "|        301|           901|\n",
      "|         22|           860|\n",
      "|        131|           760|\n",
      "|        300|           736|\n",
      "|         14|           699|\n",
      "|         48|           663|\n",
      "|         15|           661|\n",
      "|        125|           642|\n",
      "|         98|           593|\n",
      "|        128|           583|\n",
      "|         64|           582|\n",
      "+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identificando os Comerciantes com Mais Ofertas\n",
    "comerciantes_top = df_clean.groupBy(\"Merchant ID\").agg(count(\"Product ID\").alias(\"Total Products\")).orderBy(\"Total Products\", ascending=False)\n",
    "\n",
    "comerciantes_top.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56fb01ec-119f-48d3-9c5a-03cb52b7b274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+\n",
      "|  Category Label|Unique Product Titles|\n",
      "+----------------+---------------------+\n",
      "|      Microwaves|                 2107|\n",
      "|         Fridges|                 3211|\n",
      "|             TVs|                 3295|\n",
      "|Washing Machines|                 3426|\n",
      "|        Freezers|                 1911|\n",
      "| Digital Cameras|                 2411|\n",
      "|            CPUs|                 3062|\n",
      "|     Dishwashers|                 3060|\n",
      "| Fridge Freezers|                 4830|\n",
      "|   Mobile Phones|                 3682|\n",
      "+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando a função necessária\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Contando a quantidade de títulos de produtos únicos em cada categoria\n",
    "diversidade_categoria = df_clean.groupBy(\"Category Label\").agg(countDistinct(\"Product Title\").alias(\"Unique Product Titles\"))\n",
    "\n",
    "# Exibindo o resultado\n",
    "diversidade_categoria.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba6edce-e70c-481f-bb14-7ac334d32e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de partições: 2\n",
      "Total Sales: 1500\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Iniciando o SparkContext\n",
    "sc = SparkContext(\"local\", \"MapPartitionsExample\")\n",
    "\n",
    "# Exemplo de dados de vendas (ID da venda, Valor da venda, Data da venda, ID do vendedor)\n",
    "data = [(1, 100, '2021-01-01', 'A1'), (2, 200, '2021-01-02', 'A2'), \n",
    "        (3, 300, '2021-01-03', 'A3'), (4, 400, '2021-01-04', 'A4'), \n",
    "        (5, 500, '2021-01-05', 'A5')]\n",
    "\n",
    "sales_rdd = sc.parallelize(data,2)\n",
    "\n",
    "num_partitions = sales_rdd.getNumPartitions()\n",
    "print(\"Número de partições:\", num_partitions)\n",
    "\n",
    "# Função para somar vendas em cada partição\n",
    "def sum_sales(iterator):\n",
    "    yield sum(sale_amount[1] for sale_amount in iterator)\n",
    "\n",
    "# Usando mapPartitions para pré-agregar as vendas em cada partição\n",
    "partial_sums = sales_rdd.mapPartitions(sum_sales)\n",
    "\n",
    "# Usando reduce para calcular a soma total\n",
    "total_sales = partial_sums.reduce(lambda x, y: x + y)\n",
    "\n",
    "print(\"Total Sales:\", total_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869f8708-510f-48c7-b7c7-cab2fe5db4eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de partições: 2\n",
      " \n",
      "Partição: 0\n",
      "(1, 100, '2021-01-01', 'A1')\n",
      "(2, 200, '2021-01-02', 'A2')\n",
      "Partição: 1\n",
      "(3, 300, '2021-01-03', 'A3')\n",
      "(4, 400, '2021-01-04', 'A4')\n",
      "(5, 500, '2021-01-05', 'A5')\n",
      "\n",
      "Total Sales: 1500\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Iniciando o SparkContext\n",
    "sc = SparkContext(\"local\", \"PartitionExample\")\n",
    "\n",
    "# Exemplo de dados\n",
    "data = [(1, 100, '2021-01-01', 'A1'), (2, 200, '2021-01-02', 'A2'), \n",
    "        (3, 300, '2021-01-03', 'A3'), (4, 400, '2021-01-04', 'A4'), \n",
    "        (5, 500, '2021-01-05', 'A5')]\n",
    "sales_rdd = sc.parallelize(data,2)\n",
    "\n",
    "num_partitions = sales_rdd.getNumPartitions()\n",
    "print(\"Número de partições:\", num_partitions)\n",
    "print(\" \")\n",
    "\n",
    "# Função para somar vendas em cada partição\n",
    "def sum_sales(iterator):\n",
    "    yield sum(sale_amount[1] for sale_amount in iterator)\n",
    "    \n",
    "# Função para imprimir os registros em cada partição\n",
    "def show_records_in_partition(index, iterator):\n",
    "    yield f\"Partição: {index}\"\n",
    "    for record in iterator:\n",
    "        yield record\n",
    "\n",
    "# Aplicando a função para visualizar os dados em cada partição\n",
    "partitioned_records = sales_rdd.mapPartitionsWithIndex(show_records_in_partition)\n",
    "\n",
    "# Coletando e imprimindo os resultados\n",
    "for record in partitioned_records.collect():\n",
    "    print(record)\n",
    "    \n",
    "# Usando mapPartitions para pré-agregar as vendas em cada partição\n",
    "total_sales = sales_rdd.mapPartitions(sum_sales).reduce(lambda x, y: x + y)\n",
    "\n",
    "print(\"\\nTotal Sales:\", total_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b35d74c2-c03e-44e3-b5f1-89dd1e8b42a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número atual de shuffle partitions: 100\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizationExample\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Consultando a configuração\n",
    "shuffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(\"Número atual de shuffle partitions:\", shuffle_partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d421c-1c72-4a8c-9dc7-7ad20d81be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Inicializando a SparkSession com configurações otimizadas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataOptimizationExample\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Carregando os dados\n",
    "df = spark.read.csv(\"path/to/large/dataset\")\n",
    "\n",
    "# Transformações complexas\n",
    "transformed_df = df.select(col(\"col1\"), col(\"col2\")).where(col(\"col2\") > 100)\n",
    "\n",
    "# Agregação eficiente\n",
    "result_df = transformed_df.groupBy(\"col1\").count()\n",
    "\n",
    "# Salvar o resultado\n",
    "result_df.write.format(\"parquet\").save(\"path/to/output\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
